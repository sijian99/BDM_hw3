# -*- coding: utf-8 -*-
"""BDM_HW3_sw5201.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/129Jl4c43AWBfj7MaD6LwU0KDKSAUgxPw

# Apache Spark - Analyzing Food Insecurity in NYC using KeyFoods Price Catalogs

In this homework, we study the food insecurity problem by looking at the listed prices of various food items across neighborhoods in NYC. Our hypothesis is that *people living in areas with higher food insecurity problems would pay more for the same items compared to those in more secured areas*. For the scope of work, we will only assess food products from KeyFoods supermarkets, one of the top 4 Supermarket Leaders in Metro New York (according [Food Trade News 2021 report](https://www.foodtradenews.com/2021/06/29/food-trade-news-2021-market-study-issue/)). In particular, we will use the following datasets:

### **`keyfood_products.csv`**

This CSV file contains the price information about 2 million food items listed on KeyFoods stores in NYC.

|store|department|upc|product|size|price|
|--|--|--|--|--|--|
102|bakery|102-28556000000|Store Prepared - Challah Egg|1 EA|\$4.99 each|
102|bakery|102-28781600000|Store Prepared - fw Cheesecake Plain 7 Inch|1 EA|\$27.99 each|
|...|...|...|...|...|...|

The details of the columns are as follows:

|Column|Description|
|--|--|
|**store** | The unique id of each store |
|**department**| The department (or aisle) that the food item belongs to. Possible values are:<br />`'bakery'`,`'beverages'`,`'breakfast'`,`'deli'`,`'frozen'`,`'international'`,<br/>`'meatandseafood'`,`'pantry'`,`'produce'`,`'refrigerated'`, and `'snacks'`|
|**upc**|The unique id for each item in a food store. It is often in the format of `SID-XXXXXXXXXX`,<br/> where `SID` is a store id if it's specific to a store, `UPC` if it's a general product, or `'KEY'` <br/> if it's a KeyFoodsproduct. If an item doesn't have any UPC code, this field will be `N/A`.|
|**product**|This is the listed name of the product|
|**size**|The unit that the product is being sold in|
|**price**|The price text of the product as shown on their websites. This is not a number but have<br/>been verified to start with the price mark`$XX.XX`. Note that for items without price<br/>information, this field could be listed as `Price not Available`|



This is the *big data* part of the homework, where we need to use Apache Spark to process it.

### **`keyfood_nyc_stores.json`**

This JSON file contains information for all KeyFoods stores in NYC. There are a lot of details about each store, however, for this homework, we are only interested in the following fields:

|Field|Description|
|--|--|
|**name** | This is the unique id of each store, which could be crosswalk with the **store** field above |
|**communityDistrict**|The community district code that the store belongs to. It's simply a larger geographical<br/> unit comparing to a zip code. More information can be found [here](https://communityprofiles.planning.nyc.gov/).|
|**foodInsecurity**|A food insecurity score computed for the community district that the stores belong to.<br/> This value has the range of 0 to 1 with 0 being without any food insecurity rish, and 1 <br/> has the most food insecure risk.|

### **`keyfood_sample_items.csv`**

This data contains the list of 22 food items that we would like to study initially to assess our hypothesis. For each item, we have the UPC code (which needs to be generalized across store) and the item name. Here is the list:

|UPC code|Item Name|
|--|--|
|SID-20308200000|Broccoli Crowns|
|KEY-000000004094|Fresh Produce - Carrot Bunch|
|KEY-000000004062|Fresh Produce - Cucumbers|
|SID-00000004072|Fresh Produce - Potatoes Russet|
|SID-00000004131|Fresh Produce - Apples Fuji Large|
|KEY-00000004013|Produce - Orange Navel 113|
|UPC-048500001004|Tropicana - Juice Orange Pure Prem Orig|
|UPC-017400108700|Carolina - Whole Grain Brown Rice|
|UPC-016000487697|General Mills - Cherrios Multi Grain Cereal|
|UPC-073296027686|Urban Meadow - 100 Whole Wheat Bread|
|UPC-048000013651|Chicken of the Sea - Solid Wht Albacore Tuna in Oil|
|SID-20115200000|Beef - Beef Semi Bnls Chuck Stk|
|SID-28080600000|Perdue - Split Chicken Breast Fam Pack|
|UPC-073296057461|Urban Meadow - Plain Low Fat Yogurt|
|UPC-041757021443|Laughing Cow - White Cheddar Wedges|
|UPC-073296069280|Urban Meadow - Large White Eggs|
|UPC-088365000347|Cream O Land - Gallon 2% Milk|
|UPC-072940744016|Redpack - Tomato Crushed|
|UPC-051500255162|Jif - Creamy Peanut Butter|
|UPC-073296025903|Urban Meadow - Canola Oil|
|UPC-041331124461|Goya - Beans Cannelini Can|
|UPC-014500001894|Birds Eye - Spinach Leaf|

where `SID` should be replaced with the store id.

### Notes

* There are 3 tasks below. You can use Google Colab for the first 2 tasks. Task 3 requires the use of NYU HPC.

* Our big data set (`keyfood_products.csv`) is assumed to be on HDFS, and must be accessed using Spark (either as RDD or DataFrame).

* You are not allowed to collect the raw data to the notebook and process them without using Spark. However, it is okay to collect intermediate data for processing. Just try to collect as little as possible.

## Environment Setup

Since there's a number of issues with `gdown` recently, we're switching to use `curl` to download the data files.
"""

!curl -L "https://drive.google.com/uc?id=1O1U_t-cpmValVK2mjdTzcFxIbGw05vOw&confirm=t" > keyfood_sample_items.csv
!curl -L "https://drive.google.com/uc?id=1YUBKrtNV3QUz1RutMnMbJdQj7rv-Lkd5&confirm=t" > keyfood_nyc_stores.json
!curl -L "https://drive.google.com/uc?id=1f79oETtvN3NQLYPnVGhurE1UBDP4IQP-&confirm=t" > keyfood_products.csv
!pip install pyspark

# Commented out IPython magic to ensure Python compatibility.
import csv
import json
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import IPython
# %matplotlib inline
IPython.display.set_matplotlib_formats('svg')
pd.plotting.register_matplotlib_converters()
sns.set_style("whitegrid")

import pyspark
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql import types as T
sc = pyspark.SparkContext.getOrCreate()
spark = SparkSession(sc)
spark

"""## Task 1 - Visualizing Distributions of Listed Food Prices

In the first task, we would like to see how the listed prices for food items vary across stores. For each item in the sample list provided in `keyfood_sample_items.csv`, we can simply overlay a [strip plot](https://seaborn.pydata.org/generated/seaborn.stripplot.html) with a [violin plot](https://seaborn.pydata.org/generated/seaborn.violinplot.html). In addition, to better correlate the price distribution with the food insecurity risk, we will color the markers by the percentage of food insecurity (derived from the `foodInsecurity` field in `keyfood_nyc_stores.json`). An expected visualization is provided below:
"""

#@title

"""To produce the plot, we need to following data, where each row represents a listing of the sample food item from a store.

| Item Name	| Price ($) | % Food Insecurity |
|--|--|--|
|Urban Meadow - 100 Whole Wheat Bread | 2.29 | 11 |
|General Mills - Cherrios Multi Grain Cereal | 6.79 | 11 |
|Birds Eye - Spinach Leaf | 2.29 | 11 |
|Beef - Beef Semi Bnls Chuck Stk | 7.99 | 11 |
|Chicken of the Sea - Solid Wht Albacore Tuna in Oil | 2.49 | 11 |
| ... | ... | ... |

Your task is to compute the above table from the input data.

### INPUT
**You must read `keyfood_products.csv` into a Spark's DataFrame or Spark's RDD**, and process the data from there. You may assume the other two files (`keyfood_nyc_stores.json` and `keyfood_sample_items.csv`) are stored locally with your driver code.

### OUTPUT
Your output must be stored in another Spark's DataFrame or Spark's RDD, and being named as `outputTask1` at the end of the task. The column order must be the same as above (item name, price, and then % food insecurity). If your output is an RDD, each record should be a tuple of 3 elements. If your output is a Spark's DataFrame, it must have exactly 3 columns (you name them anything but they must in the same order).

### [TODO] A. Complete your code

Using either Spark's RDD or Spark's DataFrame transformations. The output must be placed in the `outputTask1` variable:

* Items must be filtered by UPC codes and names provided in the `keyfood_sample_items.csv`. UPC codes are considered equal if their numeric parts (the second portion after the `-`) are the same. For example, `SID-20308200000` is the same as `102-20308200000`, `KEY-20308200000`, etc.

* `Item Name` must be taken the sample items when there's a UPC code match (as defined above).

* `Price` should be extracted from the `price` column of `keyfood_products.csv`. The prefix `$` should be removed, and the output price should be converted to a float number (i.e. not a string).

*  `% Food Insecurity` is simply the percentage of the `foodInsecurity` score, i.e. by multiplying `foodInsecruity` by `100`.
"""

df_keyfood = spark.read.csv('keyfood_sample_items.csv')
df_keyfood = df_keyfood.filter(df_keyfood._c0!='UPC code')

sampleRDD = sc.textFile('keyfood_sample_items.csv')
sample_df = spark.read.csv('keyfood_sample_items.csv', header=True, escape='"')
sample_df = sample_df.withColumn('UPC code', F.split(F.col('UPC code'),'-').getItem(1))
sample_df = sample_df.withColumnRenamed('UPC code','ID')

list_UPCcode = df_keyfood.select('_c0').rdd.flatMap(lambda x : x).collect()
list_UPCcode = list(map(lambda x: x.split('-')[1], list_UPCcode))

productsRDD = sc.textFile('keyfood_products.csv')
df_products = spark.read.csv('keyfood_products.csv', header=True, escape='"')
df_products = df_products.withColumn('upc', F.split(F.col('upc'),'-').getItem(1))
df_product_filter = df_products.filter(F.col('upc').isin(list_UPCcode))

from pyspark.sql.types import DoubleType
rdd2 = df_product_filter.rdd.map(lambda x:(x[0], x[2], x[3], x[5].split('$')[1].split()[0]))
df_product = rdd2.toDF(['store', ' upc', 'product', 'price'])
df_product = df_product.withColumn('price', df_product.price.cast(DoubleType()))

import json
f = open('keyfood_nyc_stores.json') 
data = json.load(f)
list_store = [] 
for i in data:
    list_store.append(i)
f.close()

f = open('keyfood_nyc_stores.json')
data = json.load(f)
list_security = []
list_District = []
for i in list_store:
  list_security.append(data[i]['foodInsecurity'])
  list_District.append(data[i]['communityDistrict']) 
f.close()

from pyspark.sql.types import IntegerType
columns = ['name','communityDistrict','foodInsecurity']
df_security = spark.createDataFrame(zip(list_store, list_District,list_security), columns)
df_security = df_security.withColumn('foodInsecurity', df_security.foodInsecurity.cast(DoubleType()))
rdd3 = df_security.rdd.map(lambda x:(x[0], x[1], x[2]*100))
df_security = rdd3.toDF(['name', ' communityDistrict', 'foodInsecurity'])
df_security = df_security.withColumn('foodInsecurity', df_security['foodInsecurity'].cast(IntegerType()))

full_df = df_product.join(df_security,df_product.store ==  df_security.name,"inner")

rdd4 = full_df.rdd.map(lambda x:(x[1], x[2], x[3], x[6]))
full_df2 = rdd4.toDF(['ID','Item Name', ' Price ($)', '% Food Insecurity'])
full_df3 = full_df2.join(sample_df,full_df2.ID == sample_df.ID,"inner")

rdd5 = full_df3.rdd.map(lambda x:(x[5], x[2], x[3]))
outputTask1 = rdd5.toDF(['Item Name', ' Price ($)', '% Food Insecurity'])

# <YOUR CODE HERE>

## DO NOT EDIT BELOW
outputTask1 = outputTask1.cache()
outputTask1.count()

"""### B. Run to validate your output"""

#@title
def dfTask1(data):
    rdd = data.rdd if hasattr(data, 'rdd') else data
    if rdd.count()>10000:
        raise Exception('`outputTask1` has too many rows')
    rows = map(lambda x: (x[0], x[1], int(x[2])), rdd.collect())
    return pd.DataFrame(data=rows, columns=['Item Name','Price ($)','% Food Insecurity'])

def plotTask1(data, figsize=(8,8)):
    itemNames = pd.read_csv('keyfood_sample_items.csv')['Item Name']
    itemKey = dict(map(reversed,enumerate(itemNames)))
    df = dfTask1(data).sort_values(
        by = ['Item Name', '% Food Insecurity'],
        key = lambda x: list(map(lambda y: itemKey.get(y,y), x)))
    plt.figure(figsize=figsize)
    ax = sns.violinplot(x="Price ($)", y="Item Name", data=df, linewidth=0,
                        color='#ddd', scale='width', width=0.95)
    idx = len(ax.collections)
    sns.scatterplot(x="Price ($)", y="Item Name", hue='% Food Insecurity', data=df,
                    s=24, linewidth=0.5, edgecolor='gray', palette='YlOrRd')
    for h in ax.legend_.legendHandles: 
        h.set_edgecolor('gray')
    pts = ax.collections[idx]
    pts.set_offsets(pts.get_offsets() + np.c_[np.zeros(len(df)),
                                            np.random.uniform(-.1, .1, len(df))])
    ax.set_xlim(left=0)
    ax.xaxis.grid(color='#eee')
    ax.yaxis.grid(color='#999')
    ax.set_title('Item Prices across KeyFood Stores in NYC')
    return ax

if 'outputTask1' not in locals():
    raise Exception('There is no `outputTask1` produced in Task 1.A')

plotTask1(outputTask1);

"""## Task 2 - Finding the Highest Priced Items in Areas with Food Insecurity

Examining the plot from Task 1, we could notice many cases where product prices are higher in areas with high food insecurity. For example, the highest priced *Cream O Land - Gallon 2% Milk* is in the area with a high `% Food Insecurity` value (the right most marker has a saturated red, approximately 20%). This suggests that our hypothesis might hold. At this point, we could perform a full *Null Hypothesis Test*, but before that, we would like to expand our study beyond just the sample items.

In particular, we would like to find all products that meets all of the conditions below:

1.  Must be sold in at least 3 stores, each with a food insecurity risk of `low`, `medium`, and `high`, respectively. The risk is based on the `foodInsecurity` value of each store, and computed as follows:

|foodInsecurity|Risk Rating|
|--|--|
|<=0.09|low|
|>0.09 and <=0.13| n/a|
|>0.13 and <=0.16| medium|
|>0.16 and <=0.23| n/a|
|>0.23| high|

2. The highest priced location has the risk rating of `high`.

3. The standard deviation of the product prices must be more than `$1`, i.e. when we collect all listed prices of the product based on its UPC, and compute the standard deviation, its value should be larger than `1`.

Your task is to find the list of all UPC codes (only the second part after the `-` in `SID-XXXXXXXXXXX`) that meet such conditions along with its `department` value.

### INPUT
**You must read `keyfood_products.csv` into a Spark's DataFrame or Spark's RDD**, and process the data from there. You may assume `keyfood_nyc_stores.json` is stored locally with your driver code.

### OUTPUT
Your output must be stored in another Spark's DataFrame or Spark's RDD, and being named as `outputTask2` at the end of the task.

### [TODO] A. Complete your code

Using either Spark's RDD or Spark's DataFrame transformations. The output must be placed in the `outputTask2` variable with the following column order:

| Extracted UPC Code | Item Name | Department |
|--|--|--|
|073296027686 | Urban Meadow - 100 Whole Wheat Bread | refrigerated |
|20308200000 | Broccoli Crowns | produce |
| ... | ... | ... |

* The data must be sorted by the **Extracted UPC Code** alphabetically (i.e. as strings and not as numbers).

* **Item Name** can be taken from any of the product instance.
"""

df_sample = spark.read.option("header","true").csv("/content/keyfood_sample_items.csv")
df_sample.printSchema()
df = spark.read.option("header","true").csv("/content/keyfood_products.csv")
df.printSchema()
df_store = spark.read.json('/content/keyfood_nyc_stores.json')

df_sample = df_sample.withColumn('code', F.split(F.col('UPC code'), "-").getItem(1))
df = df.drop('size')
df = df.withColumn('code', F.split(F.col('upc'), "-").getItem(1)).drop('upc')
df = df.withColumn('price', F.regexp_extract(F.col('price'), "\d+\.\d{2}", 0))

#get all needed information
insecurityDict = { x:df_store.select(x+'.foodInsecurity').first()['foodInsecurity'] for x in df_store.columns}
callnewColsUdf = F.udf(lambda x: insecurityDict[x], T.DoubleType())
df = df.withColumn('foodInsecurity', callnewColsUdf(F.col('store')) )
df.show(5)

#creat a new column according to rating
df_rating = df.withColumn('Risk Rating', F.when((F.col("foodInsecurity") <=0.09), "low").otherwise(F.when((F.col("foodInsecurity") >0.23), "high")
                                                    .otherwise(F.when((F.col("foodInsecurity") >0.13) & (F.col("foodInsecurity") <=0.16), "medium")
                                                    .otherwise(None)))).dropna()

df_rating.show()



"""**filter by std**"""

#find the code satisfy requirement
std_price = df_rating.groupby('code').agg({'price': 'stddev'}).filter(F.col('stddev(price)') > 1)

list_std = set(std_price.select('code').rdd.flatMap(lambda x: x).collect())

#filter wanted code with their info
after_std_filter = df_rating.filter(F.col('code').isin(list_std))
after_std_filter.count()

"""**Filter by high** **rating**"""

from pyspark.sql.window import Window
from pyspark.sql.functions import col, row_number
windowDept = Window.partitionBy("code").orderBy(col("price").desc())
high_price_and_rating = after_std_filter.withColumn("row",row_number().over(windowDept)) \
  .filter(col("row") == 1).drop("row")

#find the code satisfy requirement
high_price_and_rating = high_price_and_rating.filter(F.col('Risk Rating') == 'high')
high_price_and_rating.show()

high_price_and_rating.count()

# filter again
after_high_filter = after_std_filter.join(high_price_and_rating, (high_price_and_rating.code ==  after_std_filter.code) ,"inner")

rdd_high = after_high_filter.rdd.map(lambda x:(x[0], x[1], x[2],x[3], x[4],x[6]))
after_high_filter2 = rdd_high.toDF(['store','department', 'product', 'price','code','rating'])

after_high_filter.count()

after_high_filter2.show()

"""**Filter by three different rating**"""

byRating = after_high_filter2.groupBy("code").pivot("rating").count().dropna()
byRating.show()

after_three_filter = after_high_filter2.join(byRating, (after_high_filter2.code ==  byRating.code) ,"inner")
after_three_filter.count()

#drop duplicate codes
after_three_filter = after_three_filter.dropDuplicates(['code'])
after_three_filter.count()

after_three_filter.show()

#get the final table
rdd_three = after_three_filter.rdd.map(lambda x:(x[4], x[2], x[1]))
after_three_filter2 = rdd_three.toDF(['code','product', 'department'])
after_three_filter2 = after_three_filter2.sort(F.col("code"))

outputTask2 = after_three_filter2

#<YOUR CODE HERE>

## DO NOT EDIT BELOW
outputTask2 = outputTask2.cache()
outputTask2.count()

"""### B. Run to validate your output"""

#@title
def dfTask2(data):
    rdd = data.rdd if hasattr(data, 'rdd') else data
    if rdd.count()>1000:
        raise Exception('`outputTask2` has too many rows')    
    return pd.DataFrame(data=rdd.collect(), 
                        columns=['Extracted UPC Code','Item Name','Department'])

if 'outputTask2' not in locals():
    raise Exception('There is no `outputTask2` produced in Task 2')

dfTask2(outputTask2).groupby('Department').size()

"""## Task 3 - NYU HPC

You are asked to convert your Task 1 into a single `.py` file named `BDM_HW3_NetID.py` that can be executed on Peel @ NYU HPC. Your file will be run with the following command:

```
spark-submit --executor-cores 5 --num-executor 1 --files keyfood_nyc_stores.json,keyfood_sample_items.csv BDM_HW3_NetID.py OUTPUT_FOLDER_NAME
```

Notes:
* `OUTPUT_FOLDER_NAME`: being specified by the user at run-time. You can access this variable through `sys.argv[1]` in your code. Your code must output data into this folder (e.g. through `saveAsTextFile`).
* You may assume that both `keyfood_nyc_stores.json` and `keyfood_sample_items.csv` are in the current folder. For testing purposes you may download the files into your code folder using the `curl` command at the top of the notebook.
* `keyfood_products.csv` is expected to be on HDFS at `/tmp/bdm/keyfood_products.csv`, you can use this path directly with Spark.
"""